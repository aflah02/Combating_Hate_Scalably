{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--ip=127.0.0.1\n",
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "import numpy as np\n",
    "\n",
    "policy = keras.mixed_precision.Policy(\"mixed_float16\")\n",
    "keras.mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretraining data.\n",
    "keras.utils.get_file(\n",
    "    origin=\"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "wiki_dir = os.path.expanduser(\"~/.keras/datasets/wikitext-103-raw/\")\n",
    "\n",
    "# Download vocabulary data (WordPiece vocabulary, to do sub-word tokenization)\n",
    "vocab_file = keras.utils.get_file(\n",
    "    origin=\"https://storage.googleapis.com/tensorflow/keras-nlp/examples/bert/bert_vocab_uncased.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing params.\n",
    "PRETRAINING_BATCH_SIZE = 128\n",
    "FINETUNING_BATCH_SIZE = 32\n",
    "SEQ_LENGTH = 128\n",
    "MASK_RATE = 0.25\n",
    "PREDICTIONS_PER_SEQ = 32\n",
    "\n",
    "# Model params.\n",
    "NUM_LAYERS = 3\n",
    "MODEL_DIM = 256\n",
    "INTERMEDIATE_DIM = 512\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.1\n",
    "NORM_EPSILON = 1e-5\n",
    "\n",
    "# Training params.\n",
    "PRETRAINING_LEARNING_RATE = 5e-4\n",
    "PRETRAINING_EPOCHS = 8\n",
    "FINETUNING_LEARNING_RATE = 5e-5\n",
    "FINETUNING_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wikitext-103 and filter out short lines.\n",
    "wiki_train_ds = (\n",
    "    tf.data.TextLineDataset(wiki_dir + \"wiki.train.raw\")\n",
    "    .filter(lambda x: tf.strings.length(x) > 100)\n",
    "    .batch(PRETRAINING_BATCH_SIZE)\n",
    ")\n",
    "wiki_val_ds = (\n",
    "    tf.data.TextLineDataset(wiki_dir + \"wiki.valid.raw\")\n",
    "    .filter(lambda x: tf.strings.length(x) > 100)\n",
    "    .batch(PRETRAINING_BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting sequence_length will trim or pad the token outputs to shape\n",
    "# (batch_size, SEQ_LENGTH).\n",
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab_file,\n",
    "    sequence_length=SEQ_LENGTH,\n",
    "    lowercase=True,\n",
    "    strip_accents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'tokens': <tf.Tensor: shape=(128, 128), dtype=int32, numpy=\n",
      "array([[ 7570,   103,  2271, ...,   103,  1012,  7570],\n",
      "       [ 7570, 23283,  2271, ...,   103,   103,  2023],\n",
      "       [ 1996,  2034,  3940, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  103,  1996,   103, ...,     0,     0,     0],\n",
      "       [ 3216,   103,  2083, ...,     0,     0,     0],\n",
      "       [ 9794,   103,  1045, ...,     0,     0,     0]])>, 'mask_positions': <tf.Tensor: shape=(128, 32), dtype=int64, numpy=\n",
      "array([[  1,   4,   5, ..., 115, 124, 125],\n",
      "       [  1,   8,   9, ..., 124, 125, 126],\n",
      "       [  5,   6,  12, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [  0,   2,   4, ..., 119, 120,   0],\n",
      "       [  1,   2,   3, ...,   0,   0,   0],\n",
      "       [  1,   5,   7, ...,   0,   0,   0]], dtype=int64)>}, <tf.Tensor: shape=(128, 32), dtype=int32, numpy=\n",
      "array([[ 7849,  7946,  1010, ...,  2039, 25009,  9673],\n",
      "       [ 7849, 19116, 10732, ...,  2075,  1007,  1012],\n",
      "       [ 3695, 22925,  1010, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [ 2076,  2307,  1010, ...,  3462,  1012,     0],\n",
      "       [ 2225,  2083,  4027, ...,     0,     0,     0],\n",
      "       [ 2007,  1030,  1999, ...,     0,     0,     0]])>, <tf.Tensor: shape=(128, 32), dtype=float16, numpy=\n",
      "array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 0.],\n",
      "       [1., 1., 1., ..., 0., 0., 0.],\n",
      "       [1., 1., 1., ..., 0., 0., 0.]], dtype=float16)>)\n"
     ]
    }
   ],
   "source": [
    "# Setting mask_selection_length will trim or pad the mask outputs to shape\n",
    "# (batch_size, PREDICTIONS_PER_SEQ).\n",
    "masker = keras_nlp.layers.MLMMaskGenerator(\n",
    "    vocabulary_size=tokenizer.vocabulary_size(),\n",
    "    mask_selection_rate=MASK_RATE,\n",
    "    mask_selection_length=PREDICTIONS_PER_SEQ,\n",
    "    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(inputs):\n",
    "    inputs = tokenizer(inputs)\n",
    "    outputs = masker(inputs)\n",
    "    # Split the masking layer outputs into a (features, labels, and weights)\n",
    "    # tuple that we can use with keras.Model.fit().\n",
    "    features = {\n",
    "        \"tokens\": outputs[\"tokens\"],\n",
    "        \"mask_positions\": outputs[\"mask_positions\"],\n",
    "    }\n",
    "    labels = outputs[\"mask_ids\"]\n",
    "    weights = outputs[\"mask_weights\"]\n",
    "    return features, labels, weights\n",
    "\n",
    "\n",
    "# We use prefetch() to pre-compute preprocessed batches on the fly on the CPU.\n",
    "pretrain_ds = wiki_train_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "pretrain_val_ds = wiki_val_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Preview a single input example.\n",
    "# The masks will change each time you run the cell.\n",
    "print(pretrain_val_ds.take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 128, 256)         7846400   \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " layer_normalization (LayerN  (None, 128, 256)         512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128, 256)          0         \n",
      "                                                                 \n",
      " transformer_encoder (Transf  (None, 128, 256)         527104    \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " transformer_encoder_1 (Tran  (None, 128, 256)         527104    \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " transformer_encoder_2 (Tran  (None, 128, 256)         527104    \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,428,224\n",
      "Trainable params: 9,428,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32)\n",
    "\n",
    "# Embed our tokens with a positional embedding.\n",
    "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=tokenizer.vocabulary_size(),\n",
    "    sequence_length=SEQ_LENGTH,\n",
    "    embedding_dim=MODEL_DIM,\n",
    ")\n",
    "outputs = embedding_layer(inputs)\n",
    "\n",
    "# Apply layer normalization and dropout to the embedding.\n",
    "outputs = keras.layers.LayerNormalization(epsilon=NORM_EPSILON)(outputs)\n",
    "outputs = keras.layers.Dropout(rate=DROPOUT)(outputs)\n",
    "\n",
    "# Add a number of encoder blocks\n",
    "for i in range(NUM_LAYERS):\n",
    "    outputs = keras_nlp.layers.TransformerEncoder(\n",
    "        intermediate_dim=INTERMEDIATE_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        layer_norm_epsilon=NORM_EPSILON,\n",
    "    )(outputs)\n",
    "\n",
    "encoder_model = keras.Model(inputs, outputs)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    }
   ],
   "source": [
    "# Create the pretraining model by attaching a masked language model head.\n",
    "inputs = {\n",
    "    \"tokens\": keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32),\n",
    "    \"mask_positions\": keras.Input(shape=(PREDICTIONS_PER_SEQ,), dtype=tf.int32),\n",
    "}\n",
    "\n",
    "# Encode the tokens.\n",
    "encoded_tokens = encoder_model(inputs[\"tokens\"])\n",
    "\n",
    "# Predict an output word for each masked input token.\n",
    "# We use the input token embedding to project from our encoded vectors to\n",
    "# vocabulary logits, which has been shown to improve training efficiency.\n",
    "outputs = keras_nlp.layers.MLMHead(\n",
    "    embedding_weights=embedding_layer.token_embedding.embeddings, activation=\"softmax\",\n",
    ")(encoded_tokens, mask_positions=inputs[\"mask_positions\"])\n",
    "\n",
    "# Define and compile our pretraining model.\n",
    "pretraining_model = keras.Model(inputs, outputs)\n",
    "pretraining_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=PRETRAINING_LEARNING_RATE),\n",
    "    weighted_metrics=[\"sparse_categorical_accuracy\"],\n",
    "    jit_compile=True,\n",
    ")\n",
    "\n",
    "# Pretrain the model on our wiki text dataset.\n",
    "pretraining_model.fit(\n",
    "    pretrain_ds, validation_data=pretrain_val_ds, epochs=PRETRAINING_EPOCHS,\n",
    ")\n",
    "\n",
    "# Save this base model for further finetuning.\n",
    "encoder_model.save(\"encoder_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../Data/PreprocessedData/train_preprocessed.csv\")\n",
    "df_test = pd.read_csv(\"../Data/PreprocessedData/test_preprocessed.csv\")\n",
    "df_val = pd.read_csv(\"../Data/PreprocessedData/val_preprocessed.csv\")\n",
    "\n",
    "df_train = df_train[['preprocessed_text', 'label']]\n",
    "df_test = df_test[['preprocessed_text', 'label']]\n",
    "df_val = df_val[['preprocessed_text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to tf.data.Dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((df_train['preprocessed_text'].values, df_train['label'].values))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((df_test['preprocessed_text'].values, df_test['label'].values))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((df_val['preprocessed_text'].values, df_val['label'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch and shuffle the dataset\n",
    "train_ds = train_ds.batch(FINETUNING_BATCH_SIZE).shuffle(10000)\n",
    "test_ds = test_ds.batch(FINETUNING_BATCH_SIZE)\n",
    "val_ds = val_ds.batch(FINETUNING_BATCH_SIZE).shuffle(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 128), dtype=int32, numpy=\n",
      "array([[1026, 5310, 1028, ...,    0,    0,    0],\n",
      "       [4485, 2139, 2480, ...,    0,    0,    0],\n",
      "       [4720, 2575, 2397, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [1026, 5310, 1028, ...,    0,    0,    0],\n",
      "       [1026, 5310, 1028, ...,    0,    0,    0],\n",
      "       [1026, 5310, 1028, ...,    0,    0,    0]])>, <tf.Tensor: shape=(32,), dtype=string, numpy=\n",
      "array([b'NOT', b'OFF', b'NOT', b'NOT', b'NOT', b'NOT', b'NOT', b'OFF',\n",
      "       b'OFF', b'OFF', b'OFF', b'NOT', b'NOT', b'OFF', b'NOT', b'NOT',\n",
      "       b'NOT', b'NOT', b'NOT', b'OFF', b'OFF', b'NOT', b'NOT', b'NOT',\n",
      "       b'NOT', b'NOT', b'OFF', b'OFF', b'NOT', b'NOT', b'OFF', b'NOT'],\n",
      "      dtype=object)>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess(sentences, labels):\n",
    "    return tokenizer(sentences), labels\n",
    "\n",
    "\n",
    "# We use prefetch() to pre-compute preprocessed batches on the fly on our CPU.\n",
    "finetune_train_ds = train_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "finetune_val_ds = val_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "finetune_test_ds = test_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Preview a single input example.\n",
    "print(finetune_val_ds.take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the encoder model from disk so we can restart fine-tuning from scratch.\n",
    "encoder_model = keras.models.load_model(\"encoder_model\", compile=False)\n",
    "\n",
    "# Take as input the tokenized input.\n",
    "inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32)\n",
    "\n",
    "# Encode and pool the tokens.\n",
    "encoded_tokens = encoder_model(inputs)\n",
    "pooled_tokens = keras.layers.GlobalAveragePooling1D()(encoded_tokens)\n",
    "\n",
    "# Predict an output label.\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(pooled_tokens)\n",
    "\n",
    "# Define and compile our finetuning model.\n",
    "finetuning_model = keras.Model(inputs, outputs)\n",
    "finetuning_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=FINETUNING_LEARNING_RATE),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Finetune the model for the SST-2 task.\n",
    "finetuning_model.fit(\n",
    "    finetune_train_ds, validation_data=finetune_val_ds, epochs=FINETUNING_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our tokenization into our final model.\n",
    "inputs = keras.Input(shape=(), dtype=tf.string)\n",
    "tokens = tokenizer(inputs)\n",
    "outputs = finetuning_model(tokens)\n",
    "final_model = keras.Model(inputs, outputs)\n",
    "final_model.save(\"final_model\")\n",
    "\n",
    "# This model can predict directly on raw text.\n",
    "restored_model = keras.models.load_model(\"final_model\", compile=False)\n",
    "inference_data = tf.constant([\"Terrible, no good, trash.\", \"So great; I loved it!\"])\n",
    "print(restored_model(inference_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "train_pred = final_model.predict(finetune_train_ds)\n",
    "test_pred = final_model.predict(finetune_test_ds)\n",
    "val_pred = final_model.predict(finetune_val_ds)\n",
    "\n",
    "# Convert predictions to labels\n",
    "train_pred = np.where(train_pred > 0.5, 1, 0)\n",
    "test_pred = np.where(test_pred > 0.5, 1, 0)\n",
    "val_pred = np.where(val_pred > 0.5, 1, 0)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "computeAllScores(train_pred, val_pred, test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5a87ee616be0254e3f1af9223138e3faeac65b2c9d91bc22a9fc5a4a8bd8eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
