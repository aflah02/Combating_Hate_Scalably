{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv9n6KDV_LNx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/College/Semester 5/NLP Project/NLP_Project-main/NLP_Project-main/Data/PreprocessedData/train_preprocessed.csv')"
      ],
      "metadata": {
        "id": "1NqF_nTI__mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = pd.read_csv('/content/drive/MyDrive/College/Semester 5/NLP Project/NLP_Project-main/NLP_Project-main/Data/PreprocessedData/val_preprocessed.csv')"
      ],
      "metadata": {
        "id": "5vX0PqkuFnW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = train_df['preprocessed_text'].to_list()\n",
        "train_Y = (train_df['label'].replace('OFF',1)).replace('NOT', 0).to_list()"
      ],
      "metadata": {
        "id": "0wOkywr8AoAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_X = val_df['preprocessed_text'].to_list()\n",
        "val_Y = (val_df['label'].replace('OFF',1)).replace('NOT', 0).to_list()"
      ],
      "metadata": {
        "id": "SQVD42IOFsTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10000\n",
        "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_X)"
      ],
      "metadata": {
        "id": "PDON32E3_-5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=128,\n",
        "        mask_zero=True),\n",
        "   tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "ISROsum6_Nrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(0.0001),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "j3KXwSFDDZ6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6QYHxhKLtCU",
        "outputId": "7f9cbaee-3124-4930-b1d8-aeda7a459e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         1280000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                49408     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,333,633\n",
            "Trainable params: 1,333,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x = train_X, y = train_Y, epochs=10,\n",
        "                    validation_data=(val_X, val_Y),\n",
        "                    validation_steps=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpw0EcpiE4dn",
        "outputId": "1139e76e-df11-4308-cdba-7c43bcad6e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "330/331 [============================>.] - ETA: 0s - loss: 0.6489 - accuracy: 0.6714"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r331/331 [==============================] - 27s 67ms/step - loss: 0.6489 - accuracy: 0.6710 - val_loss: 0.6236 - val_accuracy: 0.6545\n",
            "Epoch 2/10\n",
            "331/331 [==============================] - 18s 54ms/step - loss: 0.5811 - accuracy: 0.6721\n",
            "Epoch 3/10\n",
            "331/331 [==============================] - 18s 54ms/step - loss: 0.4521 - accuracy: 0.7772\n",
            "Epoch 4/10\n",
            "331/331 [==============================] - 18s 54ms/step - loss: 0.3377 - accuracy: 0.8563\n",
            "Epoch 5/10\n",
            "331/331 [==============================] - 18s 54ms/step - loss: 0.2716 - accuracy: 0.8905\n",
            "Epoch 6/10\n",
            "331/331 [==============================] - 18s 54ms/step - loss: 0.2240 - accuracy: 0.9111\n",
            "Epoch 7/10\n",
            "331/331 [==============================] - 18s 54ms/step - loss: 0.1877 - accuracy: 0.9281\n",
            "Epoch 8/10\n",
            "331/331 [==============================] - 18s 54ms/step - loss: 0.1674 - accuracy: 0.9381\n",
            "Epoch 9/10\n",
            "331/331 [==============================] - 18s 54ms/step - loss: 0.1385 - accuracy: 0.9508\n",
            "Epoch 10/10\n",
            "331/331 [==============================] - 18s 55ms/step - loss: 0.1225 - accuracy: 0.9579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pred = model.predict(train_X)\n",
        "val_pred = model.predict(val_X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5XV2x_ZIQ_9",
        "outputId": "afc6a3ab-3604-482f-f972-7b76407ac2fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "331/331 [==============================] - 6s 13ms/step\n",
            "83/83 [==============================] - 2s 13ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pred = np.where(train_pred > 0.5, 1, 0)\n",
        "val_pred = np.where(val_pred > 0.5, 1, 0)"
      ],
      "metadata": {
        "id": "OGQJ1wW1sxYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def computeAllScores(y_pred_train, y_pred_dev, train_labels, dev_labels):\n",
        "    print(\"Accuracy Train: \", accuracy_score(train_labels, y_pred_train))\n",
        "    print(\"Accuracy Dev: \", accuracy_score(dev_labels, y_pred_dev))\n",
        "    print(\"Weighted F1 Train: \", f1_score(train_labels, y_pred_train, average='weighted'))\n",
        "    print(\"Weighted F1 Dev: \", f1_score(dev_labels, y_pred_dev, average='weighted'))\n",
        "    print(\"Macro F1 Train: \", f1_score(train_labels, y_pred_train, average='macro'))\n",
        "    print(\"Macro F1 Dev: \", f1_score(dev_labels, y_pred_dev, average='macro'))\n",
        "    print(\"Micro F1 Train: \", f1_score(train_labels, y_pred_train, average='micro'))\n",
        "    print(\"Micro F1 Dev: \", f1_score(dev_labels, y_pred_dev, average='micro'))\n",
        "    print(\"Weighted Recall Train: \", recall_score(train_labels, y_pred_train, average='weighted'))\n",
        "    print(\"Weighted Recall Dev: \", recall_score(dev_labels, y_pred_dev, average='weighted'))\n",
        "    print(\"Macro Recall Train: \", recall_score(train_labels, y_pred_train, average='macro'))\n",
        "    print(\"Macro Recall Dev: \", recall_score(dev_labels, y_pred_dev, average='macro'))\n",
        "    print(\"Micro Recall Train: \", recall_score(train_labels, y_pred_train, average='micro'))\n",
        "    print(\"Micro Recall Dev: \", recall_score(dev_labels, y_pred_dev, average='micro'))\n",
        "    # Confusion Matrix\n",
        "    print(\"Confusion Matrix Train: \")\n",
        "    print(confusion_matrix(train_labels, y_pred_train))\n",
        "    print(\"Confusion Matrix Dev: \")\n",
        "    print(confusion_matrix(dev_labels, y_pred_dev))"
      ],
      "metadata": {
        "id": "O-0Gp7t-rDo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "computeAllScores(train_pred, val_pred, train_Y, val_Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV_jxJygrzy_",
        "outputId": "1fc454e9-d641-4da0-a39a-0189658aab19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Train:  0.9634629909365559\n",
            "Accuracy Dev:  0.7345166163141994\n",
            "Weighted F1 Train:  0.9631101417211528\n",
            "Weighted F1 Dev:  0.724120565231274\n",
            "Macro F1 Train:  0.9578508007409474\n",
            "Macro F1 Dev:  0.6859861715728305\n",
            "Micro F1 Train:  0.9634629909365559\n",
            "Micro F1 Dev:  0.7345166163141994\n",
            "Weighted Recall Train:  0.9634629909365559\n",
            "Weighted Recall Dev:  0.7345166163141994\n",
            "Macro Recall Train:  0.9497408843181869\n",
            "Macro Recall Dev:  0.677750450118087\n",
            "Micro Recall Train:  0.9634629909365559\n",
            "Micro Recall Dev:  0.7345166163141994\n",
            "Confusion Matrix Train: \n",
            "[[7035   72]\n",
            " [ 315 3170]]\n",
            "Confusion Matrix Dev: \n",
            "[[1493  240]\n",
            " [ 463  452]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DhPXVGBXQAJM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}