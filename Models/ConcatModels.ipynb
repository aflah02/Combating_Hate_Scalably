{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('/home/aflah20082/NLP_Project/Data/PreprocessedData/train_preprocessed.csv')\n",
    "df_test = pd.read_csv('/home/aflah20082/NLP_Project/Data/PreprocessedData/test_preprocessed.csv')\n",
    "df_val = pd.read_csv('/home/aflah20082/NLP_Project/Data/PreprocessedData/val_preprocessed.csv')\n",
    "\n",
    "# Add Dummy Labels to Test\n",
    "df_test['label'] = 0\n",
    "\n",
    "df_train = df_train[['preprocessed_text', 'label']]\n",
    "df_test = df_test[['preprocessed_text', 'label']]\n",
    "df_val = df_val[['preprocessed_text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.rename(columns={'preprocessed_text': 'text', 'label': 'label'})\n",
    "df_test = df_test.rename(columns={'preprocessed_text': 'text', 'label': 'label'})\n",
    "df_val = df_val.rename(columns={'preprocessed_text': 'text', 'label': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Label to 0 and 1\n",
    "df_train['label'] = df_train['label'].replace({'NOT': 1, 'OFF': 0})\n",
    "df_val['label'] = df_val['label'].replace({'NOT': 1, 'OFF': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09917f9f6603490a87ae6f867a909011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/558 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a52bd0baa83428e9ce5a21fdb1d53e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/824k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec4a3482e4149ca831a34f18efaea83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489b0684312c4f99b8570f4ab7799b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec22ff800ed4ed4a0003c5b0349a793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347f4e7043a74076842bc9fd54d0854f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65369f42f0da4a9e9e829ebaab849c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c179837bbd54882b2fe4ecc2e76d080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e26467a4fd4a71a201902504cf89eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa869dd9c7884983838f9728d748fd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954a31f1e59d4952bdb7646b6be0e7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/151 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2c2f85ca6b49d3871e6ac2eade877a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005278ea3ccf4d3a9a9c7bdb4136b8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/151 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707c152b7e14431eb2ddc5d29a9002df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9687fe8056404f36ad54acc0aac50c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcadfba4edb42cdaffab4ccb4c49ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/151 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "bertweet_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", force_download=True)\n",
    "bertbase_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", force_download=True)\n",
    "hatebert_tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/hateBERT\", force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0760a036b3ec491ebe704d90900b62e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/662 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b1c8ed53704c4fadc527693bc94220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68998a95192e4c80b06776767a1f7a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc692f428beb4fde9176f4dafa3e82fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/828 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c6e158684d458cbfcc7e88e64a3be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/662 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed185e94d1b431eafb8a67ed42e88e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4e467d15b241e7816fcf8c25033dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe4f316a81341d0a56f6152689420d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/828 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9242fc318f34630bf806d02d2b9b5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/662 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0f5e20fcef4fd99323d089acb43323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83467305dde460d87265a43b58ba37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bd39a9ee2b4611be344641f2dad10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/828 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset,Dataset,DatasetDict\n",
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "dataset_hf = DatasetDict({\n",
    "    'train': Dataset.from_pandas(df_train),\n",
    "    'test': Dataset.from_pandas(df_test),\n",
    "    'validation': Dataset.from_pandas(df_val),\n",
    "    'train_val': Dataset.from_pandas(pd.concat([df_train, df_val]))\n",
    "})\n",
    "\n",
    "bertweet_tokenized_dataset = dataset_hf.map(lambda examples: bertweet_tokenizer(examples['text'], truncation=True, padding='max_length'), batched=True, batch_size=16)\n",
    "bertbase_tokenized_dataset = dataset_hf.map(lambda examples: bertbase_tokenizer(examples['text'], truncation=True, padding='max_length'), batched=True, batch_size=16)\n",
    "hatebert_tokenized_dataset = dataset_hf.map(lambda examples: hatebert_tokenizer(examples['text'], truncation=True, padding='max_length'), batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70356e9e93e94d88b4bc6b28959ccb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9bc98a5f1f45179464e80975326db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c9c74aaa5443b0aae0a1846b611974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b08d111124f41589a6767cd5301ba29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0334e879842f4594936a438c18e983cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0197a4cddfa84437ae2844c32e2a29f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f105240a1744459809573c4ad08da70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce80348b553046b9825eb910ab29fde7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine the two datasets\n",
    "dataset_hf = DatasetDict({\n",
    "    'train': Dataset.from_pandas(df_train),\n",
    "    'test': Dataset.from_pandas(df_test),\n",
    "    'validation': Dataset.from_pandas(df_val),\n",
    "    'train_val': Dataset.from_pandas(pd.concat([df_train, df_val]))\n",
    "})\n",
    "\n",
    "bertweet_tokenized_dataset = dataset_hf.map(lambda examples: bertweet_tokenizer(examples['text'], truncation=True, padding='max_length'), batched=True)\n",
    "bertbase_tokenized_dataset = dataset_hf.map(lambda examples: bertbase_tokenizer(examples['text'], truncation=True, padding='max_length'), batched=True)\n",
    "\n",
    "bertweet_tokenized_dataset.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "data_collator_bertweet = DataCollatorWithPadding(tokenizer=bertweet_tokenizer)\n",
    "\n",
    "bertbase_tokenized_dataset.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "data_collator_bertbase = DataCollatorWithPadding(tokenizer=bertbase_tokenizer)\n",
    "\n",
    "hatebert_tokenized_dataset.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "data_collator_hatebert = DataCollatorWithPadding(tokenizer=hatebert_tokenizer)\n",
    "\n",
    "# Combine the two datasets\n",
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, bertweet_dataset, bertbase_dataset, hatebert_dataset):\n",
    "        self.dataset1 = bertweet_dataset\n",
    "        self.dataset2 = bertbase_dataset\n",
    "        self.dataset3 = hatebert_dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'input_ids_bertweet': self.dataset1[index]['input_ids'],\n",
    "            'attention_mask_bertweet': self.dataset1[index]['attention_mask'],\n",
    "            'input_ids_bertbase': self.dataset2[index]['input_ids'],\n",
    "            'attention_mask_bertbase': self.dataset2[index]['attention_mask'],\n",
    "            'input_ids_hatebert': self.dataset3[index]['input_ids'],\n",
    "            'attention_mask_hatebert': self.dataset3[index]['attention_mask'],\n",
    "            'label': self.dataset1[index]['label']\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset1)\n",
    "\n",
    "train_dataset = ConcatDataset(bertweet_tokenized_dataset['train'], bertbase_tokenized_dataset['train'], hatebert_tokenized_dataset['train'])\n",
    "test_dataset = ConcatDataset(bertweet_tokenized_dataset['test'], bertbase_tokenized_dataset['test'], hatebert_tokenized_dataset['test'])\n",
    "val_dataset = ConcatDataset(bertweet_tokenized_dataset['validation'], bertbase_tokenized_dataset['validation'], hatebert_tokenized_dataset['validation'])\n",
    "train_val_dataset = ConcatDataset(bertweet_tokenized_dataset['train_val'], bertbase_tokenized_dataset['train_val'], hatebert_tokenized_dataset['train_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=16)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=16)\n",
    "train_val_dataloader = DataLoader(train_val_dataset, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at GroNLP/hateBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class ConcatModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConcatModel, self).__init__()\n",
    "        self.bertweet_model = AutoModel.from_pretrained(\"vinai/bertweet-base\", config = AutoConfig.from_pretrained(\"vinai/bertweet-base\", \n",
    "                                                                                                       output_attention = True, \n",
    "                                                                                                       output_hidden_state = True ) )\n",
    "                                            \n",
    "        self.bertbase_model = AutoModel.from_pretrained(\"bert-base-uncased\", config = AutoConfig.from_pretrained(\"bert-base-uncased\",\n",
    "                                                                                                         output_attention = True, \n",
    "                                                                                                         output_hidden_state = True ) )  \n",
    "\n",
    "        self.hatebert_model = AutoModel.from_pretrained(\"GroNLP/hateBERT\", config = AutoConfig.from_pretrained(\"GroNLP/hateBERT\",\n",
    "                                                                                                         output_attention = True, \n",
    "                                                                                                         output_hidden_state = True ) )\n",
    "        # Freeze first 10 layers of bertweet\n",
    "        for param in self.bertweet_model.base_model.encoder.layer[:8].parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Freeze first 10 layers of bertbase\n",
    "        for param in self.bertbase_model.base_model.encoder.layer[:8].parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Freeze first 10 layers of hatebert\n",
    "        for param in self.hatebert_model.base_model.encoder.layer[:8].parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Dropout(p=0.1),\n",
    "            nn.Linear(768*3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, 2)\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids_bertweet, attention_mask_bertweet, input_ids_bertbase, attention_mask_bertbase, input_ids_hatebert, attention_mask_hatebert, label=None):\n",
    "        logits_a = self.bertweet_model(input_ids_bertweet, attention_mask=attention_mask_bertweet).last_hidden_state[:, 0, :].view(-1, 768)\n",
    "        logits_b = self.bertbase_model(input_ids_bertbase, attention_mask=attention_mask_bertbase).last_hidden_state[:, 0, :].view(-1, 768)\n",
    "        logits_c = self.hatebert_model(input_ids_hatebert, attention_mask=attention_mask_hatebert).last_hidden_state[:, 0, :].view(-1, 768)\n",
    "        # summed_vectors = torch.mean(torch.stack([logits_a, logits_b, logits_c]), dim=0)\n",
    "        concat_vectors = torch.cat((logits_a, logits_b, logits_c), dim=1)\n",
    "        output = self.classifier(concat_vectors)\n",
    "        # print(output.shape)\n",
    "        # Compute Loss\n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # print(output.shape)\n",
    "            # print(label.shape)\n",
    "            # print(label.view(-1).shape)\n",
    "            # print(output.view(-1, 2).shape)\n",
    "            loss = loss_fct(output.view(-1, 2), label.view(-1))\n",
    "            \n",
    "            return TokenClassifierOutput(loss=loss, logits=output, hidden_states=None, attentions=None)\n",
    "\n",
    "        \n",
    "model = ConcatModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aflah20082/anaconda3/envs/py37/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "\n",
    "num_epoch = 2\n",
    "\n",
    "num_training_steps = num_epoch * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = 10000,\n",
    "    num_training_steps = num_training_steps,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "f1 = load_metric(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8c934aa66c4391a83ca0afdced7fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a5dfe19301464f8f2eee71a332767e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 of 1324: loss = 0.6314172744750977\n",
      "Step 200 of 1324: loss = 0.5211493372917175\n",
      "Step 300 of 1324: loss = 0.5171315670013428\n",
      "Step 400 of 1324: loss = 0.3491620719432831\n",
      "Step 500 of 1324: loss = 0.48275768756866455\n",
      "Step 600 of 1324: loss = 0.286018043756485\n",
      "{'accuracy': 0.8032477341389728}\n",
      "Step 700 of 1324: loss = 0.5556113123893738\n",
      "Step 800 of 1324: loss = 0.24925510585308075\n",
      "Step 900 of 1324: loss = 0.2936851680278778\n",
      "Step 1000 of 1324: loss = 0.4179897606372833\n",
      "Step 1100 of 1324: loss = 0.34476351737976074\n",
      "Step 1200 of 1324: loss = 0.2903992831707001\n",
      "Step 1300 of 1324: loss = 0.356641560792923\n",
      "{'accuracy': 0.7794561933534743}\n"
     ]
    }
   ],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "# num_training_steps = num_epoch * len(train_dataloader)\n",
    "# warmup_steps = 0\n",
    "# num_epochs = 2\n",
    "\n",
    "# progress_bar_train = tqdm(range(num_training_steps))\n",
    "# progress_bar_eval = tqdm(range(num_epochs * len(val_dataloader)))\n",
    "\n",
    "# step = 0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "  \n",
    "#   for batch in train_dataloader:\n",
    "#       model.train()\n",
    "#       # print([type(v) for v in batch.values()])\n",
    "#       batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      \n",
    "#       outputs = model(**batch)\n",
    "#       loss = outputs.loss\n",
    "#       loss.backward()\n",
    "\n",
    "#       optimizer.step()\n",
    "#       lr_scheduler.step()\n",
    "#       optimizer.zero_grad()\n",
    "#       progress_bar_train.update(1)\n",
    "#       step += 1\n",
    "\n",
    "#       if step % 100 == 0:\n",
    "#           print(f\"Step {step} of {num_training_steps}: loss = {loss.item()}\")\n",
    "#           # Save model\n",
    "#           torch.save(model.state_dict(), f\"/home/aflah20082/NLP_Project/Models/CustomModelSaves/model_{step}.pt\")\n",
    "          \n",
    "\n",
    "#   model.eval()\n",
    "#   for batch in val_dataloader:\n",
    "#     # print(batch.keys())\n",
    "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**batch)\n",
    "\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"label\"])\n",
    "#     progress_bar_eval.update(1)\n",
    "    \n",
    "#   print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at GroNLP/hateBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "# torch.save(model.state_dict(), \"model.pt\")\n",
    "\n",
    "# Load the model\n",
    "model2 = ConcatModel().to(device)\n",
    "model2.load_state_dict(torch.load(\"/home/aflah20082/NLP_Project/Models/CustomModelSaves/model_600.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7945619335347432} {'f1': 0.7594238914051331}\n"
     ]
    }
   ],
   "source": [
    "model2.eval()\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    # print(batch.keys())\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model2(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"label\"])\n",
    "    f1.add_batch(predictions=predictions, references=batch[\"label\"])\n",
    "\n",
    "print(metric.compute(), f1.compute(average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.eval()\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model2(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the model\n",
    "import pickle\n",
    "with open('firstconcatmodel.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "import pickle\n",
    "with open('firstconcatmodel.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "ls_preds = []\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    ls_preds.append(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0], device='cuda:0'),\n",
       " tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1], device='cuda:0')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a67492e6edd839247f88539501b6e58f755504339f85783a0bf23372fdc3c03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
